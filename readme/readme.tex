% !TEX TS-program = pdflatex
% !TEX encoding = UTF-8 Unicode

% This is a simple template for a LaTeX document using the "article" class.
% See "book", "report", "letter" for other types of document.

\documentclass[english,11pt]{article} % use larger type; default would be 10pt

\usepackage[utf8]{inputenc} % set input encoding (not needed with XeLaTeX)

%%% Examples of Article customizations
% These packages are optional, depending whether you want the features they provide.
% See the LaTeX Companion or other references for full information.

%%% PAGE DIMENSIONS
\usepackage{geometry} % to change the page dimensions
\geometry{letterpaper} % or letterpaper (US) or a5paper or....
\geometry{margin=0.75in} % for example, change the margins to 2 inches all round
% \geometry{landscape} % set up the page for landscape
%   read geometry.pdf for detailed page layout information

\usepackage{graphicx} % support the \includegraphics command and options

% \usepackage[parfill]{parskip} % Activate to begin paragraphs with an empty line rather than an indent

%%% PACKAGES
\usepackage{booktabs} % for much better looking tables
\usepackage{array} % for better arrays (eg matrices) in maths
\usepackage{verbatim} % adds environment for commenting out blocks of text & for better verbatim
%\usepackage{subfig} % make it possible to include more than one captioned figure/table in a single float
% These packages are all incorporated in the memoir class to one degree or another...
\usepackage{amsthm,amsmath, amssymb}
\usepackage[authoryear]{natbib}
\usepackage[colorlinks=true,citecolor=blue, urlcolor=blue,breaklinks]{hyperref}
\usepackage{babel}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{graphicx} % support the \includegraphics command and options
\usepackage{color}
\usepackage{framed}
\usepackage{lscape}
\usepackage{rotating}
\usepackage{algorithm}
\usepackage[noend]{algpseudocode}

%%% HEADERS & FOOTERS
\usepackage{fancyhdr} % This should be set AFTER setting up the page geometry
\pagestyle{fancy} % options: empty , plain , fancy
\renewcommand{\headrulewidth}{0pt} % customise the layout...
\lhead{}\chead{}\rhead{}
\lfoot{}\cfoot{\thepage}\rfoot{}

\usepackage{amssymb,amsmath,amsfonts}
\usepackage{hyperref}
\usepackage{natbib}

\usepackage{chngcntr}
\counterwithout{figure}{section}
\counterwithout{table}{section}
%%% SECTION TITLE APPEARANCE
%\usepackage{sectsty}
%\allsectionsfont{\sffamily\mdseries\upshape} % (See the fntguide.pdf for font help)
% (This matches ConTeXt defaults)

%%% ToC (table of contents) APPEARANCE
%\usepackage[nottoc,notlof,notlot]{tocbibind} % Put the bibliography in the ToC
%\usepackage[titles,subfigure]{tocloft} % Alter the style of the Table of Contents
%\renewcommand{\cftsecfont}{\rmfamily\mdseries\upshape}
%\renewcommand{\cftsecpagefont}{\rmfamily\mdseries\upshape} % No bold!

%%% END Article customizations

%%% The "real" document content comes below...

\title{P-value Weighting package for MATLAB}
\author{Edgar Dobriban\thanks{Department of Statistics, Stanford University, \texttt{dobriban@stanford.edu}} }

%\date{} % Activate to display a given date or no date (if empty),
         % otherwise the current date is printed 

\begin{document}
\maketitle
\tableofcontents
\section{Introduction}

This MATLAB package contains open source implementations of p-value weighting methods for multiple testing, including Spjotvoll, exponential and Bayes weights \citep[proposed in][]{dobriban2015optimal}. %These are methods for improving power in multiple testing via the use of prior information.  

\begin{itemize}
\item{Version: } 0.0.1
\item{Requirements: } Tested on MATLAB R2014a and R2014b. 
\item{Author: } Edgar Dobriban
\item{License: } \verb+GPL-3+
\end{itemize}

In addition, this package contains the code to reproduce all simulation results from the paper \cite{dobriban2015optimal}. These are contained in the \verb+\Examples+ folder. The data analyses from that paper can be reproduced with the code from \verb+\Data Analysis+, see Section \ref{repro}. 

\section{Installation}

Extract the archive in any folder, say to \verb+<path>+ . The main functions are in the \verb+Code+ directory, which needs to be on the Matlab path. For instance add the following line to your Matlab startup:

$$\verb+addpath('<path>/pvalue_weighting_matlab/Code')+$$

Alternatively,  add that line to scripts that call functions in this package. An example data analysis is in the \verb+\Examples+ \verb+\Example00 - First Example\+ \verb+example.m+ file. This is described in Section \ref{example}.

This file is the main documentation for the package. To start, look at the example (Section \ref{example}) or at the methods implemented (Sections \ref{methods}, \ref{methods_plus}).

%\section{Quick usage example}
%
%
%To compute Bayes weights, call \verb+w = bayes_weights(mu,sigma,q)+, where $\mu$ and $\sigma$ are the prior estimates of the effect sizes and prior standard errors. Also \verb+q+ is the significance level at which we want to test each hypothesis.

\section{P-value weighting methods}
\label{methods}

For each p-value weighting method, we assume we observe data $T_i \sim \mathcal{N}(\mu_i, 1)$ and test each null hypothesis $H_i: \mu_i \ge 0$ against $\mu_i <0$.  The p-value for testing $H_i$ is $P_i = \Phi(T_i)$, where $\Phi$ is the normal cumulative distribution function.  For a weight vector $w \in [0,\infty)^{J}$ and significance level $q \in [0,1]$, the weighted Bonferroni procedure rejects $H_{i}$ if $P_i \le q w_i$. Usual Bonferroni corresponds to $w_i=1$.

Each p-value weighting method assumes some additional independent information about $\mu_i$, and returns a weight vector $w$. These can then be used for weighted Bonferroni or other multiple testing procedures.

\subsection{Bayes}


Bayes p-value weights can be computed using:  \verb+[w, q_star, q_thresh, c]+ \verb+= bayes_weights(eta, sigma, q)+. The inputs specify the prior distribution of the means $\mu_i $ of the test statistics as:

$$\mu_i \sim \mathcal{N}(\eta_i,\sigma_i^2), \mbox{   } i \in \{1,\ldots, J\}$$ 

where:

\begin{itemize}
\item \verb+eta+:  a vector of length J, the estimated means of test statistics, derived from the prior data
\item \verb+sigma+:  a strictly positive vector of length J, the estimated standard errors of test statistics, derived from the prior data
\item \verb+q+: The weights are optimal if each hypothesis is tested at level $q$. For instance, if we want to control the FWER globally at $0.05$, then we should use $q = 0.05/J$.
\end{itemize}

The outputs are: 
\begin{itemize}
\item \verb+w+:  the optimal weights. A non-negative vector of length J.
\item \verb+q_star+: the value $q^*$ for which the weights are optimal. This may differ slightly from the original $q$ if $q$ is large.
\item \verb+q_thresh+: the largest value of $q$ for which the weights can be computed exactly 
\item \verb+c+:  the normalizing constant produced by solving the optimization problem.
\end{itemize}

This method was proposed in \cite{dobriban2015optimal}.

\subsection{Spjotvoll}


Spjotvoll p-value weights can be computed using:  \verb+[w, c]+ \verb+= spjotvoll_weights(mu, q)+. The inputs our best guess at $\mu_i$ from the prior data:

\begin{itemize}
\item \verb+mu+:  a vector of length J, the estimated means of test statistics, derived from the prior data
\item \verb+q+: The weights are optimal if each hypothesis is tested at level $q$. For instance, if we want to control the FWER globally at $0.05$, then we should use $q = 0.05/J$.
\end{itemize}

The outputs are: 
\begin{itemize}
\item \verb+w+:  the optimal weights. A non-negative vector of length J.
\item \verb+c+:  the normalizing constant produced by solving the optimization problem.
\end{itemize}

This method was proposed by \cite{wasserman2006weighted, roeder2009genome} and independently by \cite{rubin2006method}. It was called Spjotvoll weights in  \cite{dobriban2015optimal}, in honor of \citep{spjotvoll1972optimality}.

\subsection{Exponential}

Exponential weights with can be computed as \verb+w = exp_weights(mu, beta, q)+. Here the inputs are 

\begin{itemize}
\item \verb+mu+:  a vector of length J, the estimated means of test statistics, derived from the prior data
\item \verb+beta+:  the tilt $\beta$, which defines the exponent of the weight. The weights are defined as: $w_i = \exp(\beta|\eta_i|)/c$, where $c = \sum_{i=1}^{J}  \exp(\beta |\eta_i|)$.
\item \verb+q+: The weights are optimal if each hypothesis is tested at level $q$. For instance, if we want to control the FWER globally at $0.05$, then we should use $q = 0.05/J$.
\end{itemize}

The outputs are: 
\begin{itemize}
\item \verb+w+:  the exponential weights. A non-negative vector of length J.
\end{itemize}

Exponential weights are sensitive to large means. To guard against this sensitivity, we truncate the weights larger than $1/q$ and re-distribute their excess weight among the next largest weights. 

This weighting scheme was proposed in \citep{roeder2006using}, who recommend $\beta = 2$ as a default. 
%\subsection{Monotone}

%\subsection{Replication}

\section{Other Utilities}
\label{methods_plus}

P-value weighting methods consist of two components: (1) computing the weights and (2) running a multiple testing procedure. The focus of this package is in (1), but for the user's convenience, we provide some implementations of (2) as well.

\subsection{Bonferroni Multiple Testing}


Bonferroni multiple testing is performed with \verb+[h]=bonferroni(pvals,[fwer],[report])+. Here the inputs are 

\begin{itemize}
\item \verb+pvals+:  the p-values; a vector of length J with values between 0 and 1, 
\item \verb+fwer+: (optional) The family-wise error rate (FWER) that needs to be controlled. Default is \verb+fwer = 0.05+. This is the probability of making at least one error.  
\item \verb+report+: (optional) \verb+'yes'+ or \verb+'no'+. Print to screen a report of the form \verb+'Out of 100 tests,+  \verb+10 are significant using a+  \verb+family-wise error rate of 0.05.+ Default is no.
\end{itemize}

The outputs are: 
\begin{itemize}
\item \verb+h+:  a binary vector of length $J$. The indicators of the significant tests: 1 for significant, 0 for non-significant.
\end{itemize}

For a vector of $J$ p-values $P_i$, and a FWER of $\alpha$, Bonferroni rejects the hypotheses $P_i \le \alpha/J$. For a weight vector $w \in [0,\infty)^{J}$ and significance level $q \in [0,1]$, the weighted Bonferroni procedure rejects $H_{i}$ if $P_i \le q w_i$. Usual Bonferroni corresponds to $w_i=1$. Therefore, to run weighted Bonferroni, one must call the function \verb+bonferroni+ with the weighted p-values $Q_i = P_i/w_i$. An example:

\begin{verbatim}
%assume we have a vector of p-values P, and weights w
fwer = 0.05;
P_w = P/w;
h=bonferroni(P_w,alpha);
\end{verbatim}


\section{An Example with Synthetic Data}
\label{example}

We perform an experiment with synthetic data, showing how using prior data can improve power in the current study. The code is in the \verb+\Examples+ \verb+\Example00 - First Example\+ \verb+example.m+ file. To run it, set your working directory to the folder where the script resides. We will walk through  \verb+example.m+ step by step.

The first step is to get the data. You have two options: load the data, or go though the generating steps.

\subsection{Option 1: Load the readily generated data}

The first option is to load the readily generated data. Run the second cell in the matlab script \verb+example.m+

\begin{verbatim}
load('./Data/example_data.mat', 'J','P_current','t1','t2');
\end{verbatim}

%This will lead load the same data as Option 2 generates.

\subsection{Option 2: Generate data}

The second option is to walk through the data generating process. We generate two sets of test statistics, the prior and the current data. A small fraction of the prior data holds some information about the current data. However, most prior data is noise. In our experience working with GWAS data, this is a reasonable model for association studies done on two independent samples and two distinct traits (such as cardiovascular disease and aging).

We do this by drawing from a mixture distribution. We generate a large number $J$ of tests. For each test $i$ we flip a coin $X_i$: If $X_i=1$, then the prior is meaningful, else it is noise. If the prior is meaningful, we draw a random negative $\mu_i$ and both test statistics $(T^{1}_i,T^{2}_i)$ are Gaussians centered at $\mu_i$. Else we draw two independent normal test statistics $(T^{1}_i,T^{2}_i)$. This ensures that this small fraction of the data is correlated. The code is:


\begin{verbatim}
rng(0); %set seed
J = 1e3;
mu = - 2*abs(randn(J,1));
frac_sig = 0.1;
X = binornd(1,frac_sig,J,1);
t1 = normrnd(X.*mu,1);
t2 = normrnd(X.*mu,1);
\end{verbatim}

The data that this generates shows the desired patter, as can be seen on a scatterplot. Most pairs have no correlation, but there is a small fraction that does.

The p-values for the one-sided tests $\mu_i = 0 $ vs $\mu_i <0$ utilizing only the current data are $P_i = \Phi(T^{2}_i)$.


\subsection{Visualize Data}

We should now have the variables \verb+t1,t2,P_current,J+ in memory. Next we plot a scatter of the prior and current test statistics:

\begin{figure}[ht!]
\centering
\includegraphics[scale=0.6]{"../Data/Synthetic_Data_example"}
\caption{Scatter Plot of the Prior and Current Effects}
\label{scatter_prior_current}
\end{figure}

There is only a weak global correlation between the two effects. However, there is a significant correlation in the tails. 


\subsection{Data Analysis}

We want to test the hypotheses $\mu_i = 0 $ against $\mu_i <0$ for each $i$ utilizing the current data $T^{2}_i$. The simplest way is by Bonferroni-corrected multiple testing. We choose an uncorrected significance level $\alpha=0.05$ and call the \verb+bonferroni(...)+ function on the p-values.

\begin{verbatim}
alpha = 0.05; report = 'yes';
h_u=bonferroni(P_current,alpha,report);
\end{verbatim}
The output should be:

\begin{verbatim}
Out of 1000 tests, 7 are significant using a  family-wise error rate  of 0.050000.
\end{verbatim}

Bonferroni leads to 7 significant test statistics. Alternatively, one can do p-value weighting. For this, we use $T^1$ as prior information. We set the prior standard errors $\sigma=1$ in this example. More detailed discussion on the choice of $\sigma$ can be found in \citep{dobriban2015optimal}. As explained earlier, we set $q  = 0.05/J$. Then we compute the weights. Finally, we run weighted Bonferroni on the weighted p-values $P'_i = P_i/w_i$. This is the code that accomplishes it:


\begin{verbatim}
q =alpha/J; %expected fraction of false rejections under 'null'
sigma = ones(J,1);
w = informed_weights(t1,sigma,q);
P_wr = P_current./w;
h_r=bonferroni(P_wr,alpha,report);
\end{verbatim}

We should get the following output:

\begin{verbatim}
Out of 1000 tests, 12 are significant using a  family-wise error rate  of 0.050000.
\end{verbatim}

Hence, in this example weighting increases the number of significant hits from 7 to 12.

\subsection{Post-Analysis}

One can get some insight into the procedure by examining which hypotheses were declared significant by the two methods. Typing

\begin{verbatim}
find(h_u==1)
find(h_r==1)
\end{verbatim}

reveals that the significant hypotheses were:

\begin{verbatim}
ans =

    35
    84
   121
   563
   596
   734
   740


ans =

    35
    84
   121
   188
   221
   429
   563
   596
   645
   656
   734
   740
\end{verbatim}

In this particular case weighting leads to a strict increase in power, selecting an addional 5 hypotheses. Taking 429 as an example we see that its P-value in the current data equals \verb+P_current(429)=  1.0778e-04+, corresponding to a z-score \verb+t2(429)=  -3.7001+, and this is not enough for it to be significant since the threshold is $0.05/1000=5e-5$. However, it gets assigned a large weight  \verb+w(429)=  8.9348+, because its prior  effect is large, \verb+t1(429)= -4.2573+; so it's selected after weighting.

Another insight can be gained from plotting the weights as a function of the prior mean. We see that the weights are non-monotonic as a function of the prior mean. Indeed they place a large mass on the middle means between (-6 and -2).


\begin{figure}[ht!]
\centering
\includegraphics[scale=0.6]{"../Data/weights"}
\caption{Plot of weights}
\label{scatter_prior_current}
\end{figure}

\section{Reproducing Data Analysis Results}
\label{repro}

\subsection{Pipeline}

To reproduce the data analysis results from the \cite{dobriban2015optimal} paper, we provide the pipeline used to generate those results. This pipeline can be used to efficiently and reproducibly perform standard data analyses with p-value weighting of GWAS data.  It has the following features:

\begin{itemize}
\item High-level functions that perform a standard set of analyses: The user specifies the prior and current data sets, and the methods described in our paper are automatically performed.

\item Customizable parameters: The user can can change many aspects of the analysis by specifying appropriate parameters. For instance, it is possible to change the parameters of the weighting schemes.

\item Reproducibility: The results of analyses are recorded in a standard format and directory structure, which enable a reproducible analysis.

\item Extensibility:  It is possible to add new data sets, weighting schemes, and perform custom workflows. Some of this will require writing new code.
\end{itemize}

\subsection{Reproducing results}

Please follow these steps:

\begin{itemize}
\item You first need to download and process the original data files. Due to data access policies these could not be included in the package.
\item To download data files, go to \verb+Data/Raw/[Data Set Name]+ and consult the description.txt file for the description web link for the appropriate data sets.
\begin{enumerate}
\item Download and unpack each data set into its own \verb+Data/Raw/[Data Set Name]+ folder
\item Run the MATLAB scripts in the \verb+Data/Raw/[Data Set Name]/Code+ folder to process the raw data into MATLAB files. They will be deposited in \verb+Data/Raw/+.
\item Repeat the steps above for each data set.
\end{enumerate}
\item To reproduce or change existing analyses go to the folder \verb+Data Analysis/[AnalysisName]+ and run or modify analysis.m
\end{itemize}

It is also possible to extend the pipeline: 
\begin{itemize}
\item Add new analyses (for instance on newly specified pairs of GWAS data sets) by creating a new folder \verb+Analysis/[AnalysisName]+
\item Add new data sets by creating a new folder \verb+Data/Raw/[DataName]/+ where you can copy the data for a first processing. Using the existing data sets as a template, process the new data into a MATLAB format, which will then be saved in the folder \verb+Data/Processed+.
\end{itemize}

Note: The change directory statement in analysis.m is configured to the author's local machine and should be modified accordingly.

\bibliography{weighted}
\bibliographystyle{apalike}

\end{document}
